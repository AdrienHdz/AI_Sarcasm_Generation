{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Estefan : sarcasm_gen.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Un5zMSV2woQr","colab_type":"text"},"source":["** IMPORTS **"]},{"cell_type":"code","metadata":{"id":"wXbQyLmowoQ4","colab_type":"code","outputId":"33d36b3b-e00e-456b-bdc7-0b05f4ed5d2d","executionInfo":{"status":"ok","timestamp":1576628812638,"user_tz":300,"elapsed":2163,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# # https://deeplearningcourses.com/c/deep-learning-advanced-nlp\n","# get the data at: http://www.manythings.org/anki/\n","from __future__ import print_function, division\n","from builtins import range, input\n","# Note: you may need to update your version of future\n","# sudo pip install -U future\n","\n","import os, sys\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import IPython.display as display\n","\n","import keras.backend as K\n","if len(K.tensorflow_backend._get_available_gpus()) > 0:\n","  from keras.layers import CuDNNLSTM as LSTM\n","  from keras.layers import CuDNNGRU as GRU\n","from keras.callbacks import Callback\n","from keras.callbacks import LearningRateScheduler\n","from keras.models import load_model\n","from keras import regularizers\n","\n","\n","import math\n","from numpy.random import seed\n","seed(9012)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2dk65VE-DdI3","colab_type":"code","outputId":"1fe69d5a-3924-45c3-92cb-54c0894b0f81","executionInfo":{"status":"ok","timestamp":1576628812771,"user_tz":300,"elapsed":2281,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import gc\n","gc.collect()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"vrN3emf6SV3f","colab_type":"code","colab":{}},"source":["# SELECT THE MODEL OR LEAVE THE STRING EMPTY WITH \"\"\n","model_selected = \"model_20191117-233555.h5\"\n","\n","dic_model = {\"model_20191116-012401.h5\" :{  \"BATCH_SIZE\" : 64,\n","                                            \"EPOCHS\" : 45,   \n","                                            \"LATENT_DIM\" : 256 ,\n","                                            \"NUM_SAMPLES\": 20000  ,\n","                                            \"MAX_SEQUENCE_LENGTH\" : 10,\n","                                            \"MAX_NUM_WORDS\" : 20000,\n","                                            \"EMBEDDING_DIM\" : 100,\n","                                            \"SUBREDDIT\" : None,\n","                                            \"LR_DECAY\": False,\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1ykKSKPRB5N1AoLntB3SUyVSP7-z5xluM\", #glove.twitter.27B.100d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=1ixxHuOe185FJNwHqE_FYx67k1jQdotzR\"},\n","             \"model_20191117-191352.h5\" :{  \"BATCH_SIZE\" : 32,\n","                                            \"EPOCHS\" : 20,   \n","                                            \"LATENT_DIM\" : 128 ,\n","                                            \"NUM_SAMPLES\" : 20000  ,\n","                                            \"MAX_SEQUENCE_LENGTH\" : 10,\n","                                            \"MAX_NUM_WORDS\" : 20000,\n","                                            \"EMBEDDING_DIM\" : 100,\n","                                            \"SUBREDDIT\" : None,\n","                                            \"LR_DECAY\": False,\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1ykKSKPRB5N1AoLntB3SUyVSP7-z5xluM\", #glove.twitter.27B.100d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=1EZsk2nQs5T1uGSpIHkycWYqcNsI2_vDt\"},\n","              \"model_20191117-233555.h5\" :{  \"BATCH_SIZE\" : 64,\n","                                              \"EPOCHS\" : 45,   \n","                                              \"LATENT_DIM\" : 256 ,\n","                                              \"NUM_SAMPLES\": 20000  ,\n","                                              \"MAX_SEQUENCE_LENGTH\" : 10,\n","                                              \"MAX_NUM_WORDS\" : 20000,\n","                                              \"EMBEDDING_DIM\" : 100,\n","                                              \"SUBREDDIT\" : None,\n","                                              \"LR_DECAY\": True, # 0.9\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1ykKSKPRB5N1AoLntB3SUyVSP7-z5xluM\", #glove.twitter.27B.100d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=13kyuAcoULzFx2Hy3KOmlSDthGHXgdVMp\"},\n","              \"model_20191121-014934.h5\" :{  \"BATCH_SIZE\" : 64,\n","                          \"EPOCHS\" : 30,   \n","                          \"LATENT_DIM\" : 256 ,\n","                          \"NUM_SAMPLES\": 20000  ,\n","                          \"MAX_SEQUENCE_LENGTH\" : 10,\n","                          \"MAX_NUM_WORDS\" : 20000,\n","                          \"EMBEDDING_DIM\" : 200,\n","                          \"SUBREDDIT\" : None,\n","                          \"LR_DECAY\": True,\n","                          \"BIDIRECTIONAL\" : False,\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1V7tEAWJoLdl9Ul6h_glZqch5BjcU8Tqf\", #glove.twitter.27B.200d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=1yzzZEAiHx-CH_pFTsPWTt379hBaIxTfD\"},\n","              \"model_20191120-235108.h5\" :{  \"BATCH_SIZE\" : 64,\n","                                              \"EPOCHS\" : 45,   \n","                                              \"LATENT_DIM\" : 256 ,\n","                                              \"NUM_SAMPLES\": 20000  ,\n","                                              \"MAX_SEQUENCE_LENGTH\" : 10,\n","                                              \"MAX_NUM_WORDS\" : 20000,\n","                                              \"EMBEDDING_DIM\" : 300,\n","                                              \"SUBREDDIT\" : None,\n","                                              \"LR_DECAY\": True,\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1W9FeIDZcbUPFzk58bZ4Yw9nj2FX90OjJ\", #glove.6B.300d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=1kTJrPal6ioJS6wDuWAdaIOgJGXSCNYeb\"},\n","             \"model_20191121-160547.h5\" :{   \"BATCH_SIZE\" : 64,\n","                          \"EPOCHS\" : 30,   \n","                          \"LATENT_DIM\" : 128 ,\n","                          \"NUM_SAMPLES\": 20000  ,\n","                          \"MAX_SEQUENCE_LENGTH\" : 10,\n","                          \"MAX_NUM_WORDS\" : 20000,\n","                          \"EMBEDDING_DIM\" : 200,\n","                          \"SUBREDDIT\" : None,\n","                          \"LR_DECAY\": True,#0.95\n","                          \"BIDIRECTIONAL\" : False,\n","                                            \"GLOVE_LINK\": \"https://drive.google.com/open?id=1W9FeIDZcbUPFzk58bZ4Yw9nj2FX90OjJ\", #glove.6B.300d.txt\n","                                            \"LINK\" : \"https://drive.google.com/open?id=1kTJrPal6ioJS6wDuWAdaIOgJGXSCNYeb\"},\n","             }\n","selected_dic_model = dic_model.get(model_selected)\n","if not model_selected:\n","  selected_dic_model = {   \"BATCH_SIZE\" : 64,\n","                          \"EPOCHS\" : 45,   \n","                          \"LATENT_DIM\" : 256 ,\n","                          \"NUM_SAMPLES\": 20000  ,\n","                          \"MAX_SEQUENCE_LENGTH\" : 10,\n","                          \"MAX_NUM_WORDS\" : 20000,\n","                          \"EMBEDDING_DIM\" : 100,\n","                          \"SUBREDDIT\" : None,\n","                          \"LR_DECAY\": True, # 0.9\n","                            \"RNN\": \"LSTM\",\n","                          \"GLOVE_LINK\": \"https://drive.google.com/open?id=1ykKSKPRB5N1AoLntB3SUyVSP7-z5xluM\",\n","                          \"LINK\" : None}\n","\n","\n","# some config\n","BATCH_SIZE = selected_dic_model.get(\"BATCH_SIZE\")\n","EPOCHS = selected_dic_model.get(\"EPOCHS\")\n","LATENT_DIM = selected_dic_model.get(\"LATENT_DIM\")\n","NUM_SAMPLES = selected_dic_model.get(\"NUM_SAMPLES\")\n","MAX_SEQUENCE_LENGTH = selected_dic_model.get(\"MAX_SEQUENCE_LENGTH\")\n","MAX_NUM_WORDS = selected_dic_model.get(\"MAX_NUM_WORDS\")\n","EMBEDDING_DIM = selected_dic_model.get(\"EMBEDDING_DIM\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwQg7Is9zEcp","colab_type":"code","outputId":"efeff4d0-748d-4ff8-d9f0-0650c2b4bfb1","executionInfo":{"status":"ok","timestamp":1576628857050,"user_tz":300,"elapsed":46544,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sqRVFrZYzR2j","colab_type":"code","outputId":"84ecbb7a-c9c7-4c9f-dd25-155fc9b1d267","executionInfo":{"status":"ok","timestamp":1576628860854,"user_tz":300,"elapsed":50338,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["link = \"https://drive.google.com/open?id=1uH4pJhT7SX6lz-IALta41WTBJ1Y_k7oS\"\n","fluff, id = link.split('=')\n","print (id) # Verify that you have everything after '='\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('sarcasm_only.csv') "],"execution_count":5,"outputs":[{"output_type":"stream","text":["1uH4pJhT7SX6lz-IALta41WTBJ1Y_k7oS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T9bZ34i6woRG","colab_type":"code","colab":{}},"source":["\n","\n","# load in the data\n","data = pd.read_csv('sarcasm_only.csv',dtype=str)\n","data_clean = data[data['label'] == \"1\"]\n","data_clean = data_clean.astype(str)\n","data_clean.drop('ups', axis=1, inplace=True)\n","data_clean.drop('downs', axis=1, inplace=True)\n","data_clean.drop('Unnamed: 0', axis=1, inplace=True)\n","data_clean.drop('author', axis=1, inplace=True)\n","data_clean.drop('created_utc', axis=1, inplace=True)\n","data_clean.drop('date', axis=1, inplace=True)\n","data_clean.drop('label', axis=1, inplace=True)\n","\n","def lower(x):\n","  return(x.lower())\n","data_clean['comment'] = data_clean.apply(lambda x: lower(x[\"comment\"]), axis=1)\n","data_clean['parent_comment'] = data_clean.apply(lambda x: lower(x[\"parent_comment\"]), axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqD9fFeB4dBf","colab_type":"code","outputId":"444a5fd6-aaf0-46e7-9d54-cc1d300cb879","executionInfo":{"status":"ok","timestamp":1576628877907,"user_tz":300,"elapsed":67377,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["co = data_clean[\"comment\"].str.contains(\"the downvotes are because you \",na=False)\n","data_clean[co][\"comment\"].values"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([\"hahahaha oh man, i'm guessing the downvotes are because you didn't end that with\"],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"FCySiOwFlX93","colab_type":"code","colab":{}},"source":["appos = {\n","\"aren't\" : \"are not\",\n","\"can't\" : \"cannot\",\n","\"couldn't\" : \"could not\",\n","\"didn't\" : \"did not\",\n","\"doesn't\" : \"does not\",\n","\"don't\" : \"do not\",\n","\"hadn't\" : \"had not\",\n","\"hasn't\" : \"has not\",\n","\"haven't\" : \"have not\",\n","\"he'd\" : \"he would\",\n","\"he'll\" : \"he will\",\n","\"he's\" : \"he is\",\n","\"i'd\" : \"I would\",\n","\"i'd\" : \"I had\",\n","\"i'll\" : \"I will\",\n","\"i'm\" : \"I am\",\n","\"isn't\" : \"is not\",\n","\"it's\" : \"it is\",\n","\"it'll\":\"it will\",\n","\"i've\" : \"I have\",\n","\"let's\" : \"let us\",\n","\"mightn't\" : \"might not\",\n","\"mustn't\" : \"must not\",\n","\"shan't\" : \"shall not\",\n","\"she'd\" : \"she would\",\n","\"she'll\" : \"she will\",\n","\"she's\" : \"she is\",\n","\"shouldn't\" : \"should not\",\n","\"that's\" : \"that is\",\n","\"there's\" : \"there is\",\n","\"they'd\" : \"they would\",\n","\"they'll\" : \"they will\",\n","\"they're\" : \"they are\",\n","\"they've\" : \"they have\",\n","\"we'd\" : \"we would\",\n","\"we're\" : \"we are\",\n","\"weren't\" : \"were not\",\n","\"we've\" : \"we have\",\n","\"what'll\" : \"what will\",\n","\"what're\" : \"what are\",\n","\"what's\" : \"what is\",\n","\"what've\" : \"what have\",\n","\"where's\" : \"where is\",\n","\"who'd\" : \"who would\",\n","\"who'll\" : \"who will\",\n","\"who're\" : \"who are\",\n","\"who's\" : \"who is\",\n","\"who've\" : \"who have\",\n","\"won't\" : \"will not\",\n","\"wouldn't\" : \"would not\",\n","\"you'd\" : \"you would\",\n","\"you'll\" : \"you will\",\n","\"you're\" : \"you are\",\n","\"you've\" : \"you have\",\n","\"'re\": \" are\",\n","\"wasn't\": \"was not\",\n","\"we'll\":\" will\",\n","\"didn't\": \"did not\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeyoaP4tlM3Z","colab_type":"code","outputId":"fc165274-c086-438d-a989-8689700510a3","executionInfo":{"status":"ok","timestamp":1576628894493,"user_tz":300,"elapsed":83949,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Negation handling\n","def reform(sentence):\n","    words = sentence.split()\n","    reformed = [appos[word] if word in appos else word for word in words]\n","    reformed = \" \".join(reformed) \n","    return(reformed)\n","data_clean['comment'] = data_clean.apply(lambda x: reform(x[\"comment\"]), axis=1)\n","data_clean['parent_comment'] = data_clean.apply(lambda x: reform(x[\"parent_comment\"]), axis=1)\n","data_clean.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>subreddit</th>\n","      <th>score</th>\n","      <th>parent_comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>but they will have all those reviews!</td>\n","      <td>ProductTesting</td>\n","      <td>0</td>\n","      <td>the dumb thing is, they are risking their sell...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wow it is totally unreasonable to assume that ...</td>\n","      <td>politics</td>\n","      <td>2</td>\n","      <td>clinton campaign accuses fbi of 'blatant doubl...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ho ho ho... but melania said that there is no ...</td>\n","      <td>politics</td>\n","      <td>8</td>\n","      <td>anyone else think that it was interesting the ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i cannot wait until @potus starts a twitter wa...</td>\n","      <td>politics</td>\n","      <td>3</td>\n","      <td>here's what happens when obama gives up his tw...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gotta love the teachers who give exams on the ...</td>\n","      <td>CFBOffTopic</td>\n","      <td>3</td>\n","      <td>monday night drinking thread brought to you by...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             comment  ...                                     parent_comment\n","0              but they will have all those reviews!  ...  the dumb thing is, they are risking their sell...\n","1  wow it is totally unreasonable to assume that ...  ...  clinton campaign accuses fbi of 'blatant doubl...\n","2  ho ho ho... but melania said that there is no ...  ...  anyone else think that it was interesting the ...\n","3  i cannot wait until @potus starts a twitter wa...  ...  here's what happens when obama gives up his tw...\n","4  gotta love the teachers who give exams on the ...  ...  monday night drinking thread brought to you by...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"7O4ODc03lK1K","colab_type":"code","colab":{}},"source":["\n","def count(x):\n","  return len(x.split())\n","data_clean['words_comment'] = data_clean.apply(lambda x: count(x[\"comment\"]), axis=1)\n","data_clean['words_parent_comment'] = data_clean.apply(lambda x: count(x[\"parent_comment\"]), axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpZlHJcz79YB","colab_type":"code","outputId":"967907ab-7d1e-4e12-d39e-1a14bda40cde","executionInfo":{"status":"ok","timestamp":1576628908844,"user_tz":300,"elapsed":98286,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["co = data_clean[\"comment\"].str.contains(\"the downvotes are because you \",na=False)\n","data_clean[co][\"comment\"].values"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['hahahaha oh man, I am guessing the downvotes are because you did not end that with'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"Fe1J4uUhgjVa","colab_type":"code","outputId":"002999c7-78d7-450c-8684-c180f9925341","executionInfo":{"status":"ok","timestamp":1576628908847,"user_tz":300,"elapsed":98279,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["short_sentences = (data_clean['words_comment'] <= selected_dic_model.get(\"MAX_SEQUENCE_LENGTH\")) & (data_clean['words_parent_comment'] <= selected_dic_model.get(\"MAX_SEQUENCE_LENGTH\"))\n","(data_clean[short_sentences].info())"],"execution_count":12,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 117710 entries, 5 to 505406\n","Data columns (total 6 columns):\n","comment                 117710 non-null object\n","subreddit               117710 non-null object\n","score                   117710 non-null object\n","parent_comment          117710 non-null object\n","words_comment           117710 non-null int64\n","words_parent_comment    117710 non-null int64\n","dtypes: int64(2), object(4)\n","memory usage: 6.3+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WbvU96d3Nikz","colab_type":"code","cellView":"both","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_thdMV8fH7T","colab_type":"code","outputId":"65b80b5b-e410-413c-9bf5-aa9198bd9521","executionInfo":{"status":"ok","timestamp":1576628911308,"user_tz":300,"elapsed":100726,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Where we will store the data\n","input_texts = [] # sentence in original language\n","target_texts = [] # sentence in target language\n","target_texts_inputs = [] # sentence in target language offset by 1\n","if selected_dic_model.get(\"SUBREDDIT\") is not None:\n","  grouped = data_clean[short_sentences].groupby('subreddit')\n","  for subreddit_name, df_sub in grouped:\n","      if subreddit_name == \"politics\":\n","          for index, row in df_sub.iterrows():\n","              # recall we'll be using teacher forcing\n","              target_text = row['comment'] + ' <eos>'\n","              target_text_input = '<sos> ' + row['comment']\n","                    \n","              input_texts.append(row['parent_comment'])\n","              target_texts.append(target_text)\n","              target_texts_inputs.append(target_text_input)\n","  print(\"num samples:\", len(input_texts))\n","\n","else :\n","  data_clean = data_clean[short_sentences].sort_values('score',ascending = False).head(NUM_SAMPLES)\n","  for index, row in data_clean.iterrows():\n","      # recall we'll be using teacher forcing\n","      target_text = row['comment'] + ' <eos>'\n","      target_text_input = '<sos> ' + row['comment']\n","            \n","      input_texts.append(row['parent_comment'])\n","      target_texts.append(target_text)\n","      target_texts_inputs.append(target_text_input)\n","  print(\"num samples:\", len(input_texts))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["num samples: 20000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"evlf6GhUwoRL","colab_type":"code","colab":{}},"source":["# tokenize the inputs\n","tokenizer_inputs = Tokenizer()\n","tokenizer_inputs.fit_on_texts(input_texts)\n","input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oakza8HWwoRO","colab_type":"code","outputId":"a930f3c5-9793-4f84-f128-8f3f158cfa85","executionInfo":{"status":"ok","timestamp":1576628911889,"user_tz":300,"elapsed":101293,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# get the word to index mapping for input language\n","word2idx_inputs = tokenizer_inputs.word_index\n","print(\"%s mots (tokens) uniques à l'entrée\" % len(word2idx_inputs))\n","\n","# determine maximum length input sequence\n","max_len_input = max(len(s) for s in input_sequences)\n","print(\"La phrase la plus longue contient %s mots à l'entrée\" % max_len_input)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["19815 mots (tokens) uniques à l'entrée\n","La phrase la plus longue contient 21 mots à l'entrée\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5sTMEN_vwoRU","colab_type":"code","colab":{}},"source":["\n","# tokenize the outputs\n","# don't filter out special characters\n","# otherwise <sos> and <eos> won't appear\n","tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n","tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n","target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n","target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9eapjpdFwoRZ","colab_type":"code","outputId":"2529425c-fb2d-4ee8-bc83-733c56536135","executionInfo":{"status":"ok","timestamp":1576628912563,"user_tz":300,"elapsed":101952,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# get the word to index mapping for output language\n","word2idx_outputs = tokenizer_outputs.word_index\n","print(\"%s mots (tokens) uniques à la sortie\" % len(word2idx_outputs))\n","# store number of output words for later\n","# remember to add 1 since indexing starts at 1\n","num_words_output = len(word2idx_outputs) + 1\n","\n","# determine maximum length output sequence\n","max_len_target = max(len(s) for s in target_sequences)\n","print(\"La phrase la plus longue contient %s mots à la sortie\" % max_len_target)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["24342 mots (tokens) uniques à la sortie\n","La phrase la plus longue contient 11 mots à la sortie\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6AvCXjDKwoRc","colab_type":"code","outputId":"aa8f8688-ed69-4122-cdfb-37ca1ab89360","executionInfo":{"status":"ok","timestamp":1576628912779,"user_tz":300,"elapsed":102156,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# pad the sequences\n","encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n","print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n","print(\"encoder_inputs[0]:\", encoder_inputs[0])\n","\n","decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n","print(\"decoder_inputs[0]:\", decoder_inputs[0])\n","print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n","\n","decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n","\n","\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["encoder_inputs.shape: (20000, 21)\n","encoder_inputs[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0 1013\n","  420   38  184    3  292   40  240]\n","decoder_inputs[0]: [   2  764 7319    0    0    0    0    0    0    0    0]\n","decoder_inputs.shape: (20000, 11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5DhGN__l3yIG","colab_type":"code","outputId":"2621c6f4-a51a-4875-9434-6e2c9f928c4a","executionInfo":{"status":"ok","timestamp":1576628930621,"user_tz":300,"elapsed":119988,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["link = selected_dic_model.get(\"GLOVE_LINK\")\n","fluff, id = link.split('=')\n","print (id) # Verify that you have everything after '='\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('glove.txt') "],"execution_count":19,"outputs":[{"output_type":"stream","text":["1ykKSKPRB5N1AoLntB3SUyVSP7-z5xluM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4yIMAFy1woRf","colab_type":"code","outputId":"97a1cbf5-66d8-4da8-ec04-fda97cf229b9","executionInfo":{"status":"ok","timestamp":1576628964795,"user_tz":300,"elapsed":154150,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# store all the pre-trained word vectors\n","print('Loading word vectors...')\n","word2vec = {}\n","with open('glove.txt',encoding=\"utf8\")as f:\n","  # is just a space-separated text file in the format:\n","  # word vec[0] vec[1] vec[2] ...\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vec = np.asarray(values[1:], dtype='float32')\n","    word2vec[word] = vec\n","print('Found %s word vectors.' % len(word2vec))\n","\n","\n","\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Loading word vectors...\n","Found 1193514 word vectors.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sb3xUm1VQ0Te","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhpRgk1WwoRl","colab_type":"code","outputId":"a1656e6e-d46d-423d-f476-6da15b9d91ac","executionInfo":{"status":"ok","timestamp":1576628980368,"user_tz":300,"elapsed":169709,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# prepare embedding matrix\n","print('Filling pre-trained embeddings...')\n","num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n","embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","for word, i in word2idx_inputs.items():\n","  if i < MAX_NUM_WORDS:\n","    embedding_vector = word2vec.get(word)\n","    if embedding_vector is not None:\n","      # words not found in embedding index will be all zeros.\n","      embedding_matrix[i] = embedding_vector\n","\n","# create embedding layer\n","embedding_layer = Embedding(\n","  num_words,\n","  EMBEDDING_DIM,\n","  weights=[embedding_matrix],\n","  input_length=max_len_input,\n","  #trainable=True\n",")\n","\n","\n","# create targets, since we cannot use sparse\n","# categorical cross entropy when we have sequences\n","decoder_targets_one_hot = np.zeros(\n","  (\n","    len(input_texts),\n","    max_len_target,\n","    num_words_output\n","  ),\n","  dtype='float32'\n",")\n","\n","# assign the values\n","for i, d in enumerate(decoder_targets):\n","  for t, word in enumerate(d):\n","    decoder_targets_one_hot[i, t, word] = 1\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Filling pre-trained embeddings...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4gkjQ7RzcCL2","colab_type":"code","colab":{}},"source":["# Matplotlib config\n","plt.ioff()\n","plt.rc('image', cmap='gray_r')\n","plt.rc('grid', linewidth=1)\n","plt.rc('xtick', top=False, bottom=False, labelsize='large')\n","plt.rc('ytick', left=False, right=False, labelsize='large')\n","plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n","plt.rc('text', color='a8151a')\n","plt.rc('figure', facecolor='F0F0F0', figsize=(16,9))\n","\n","def plot_learning_rate(lr_func, epochs):\n","  xx = np.arange(epochs+1, dtype=np.float)\n","  y = [lr_decay(x) for x in xx]\n","  fig, ax = plt.subplots(figsize=(9, 6))\n","  ax.set_xlabel('epochs')\n","  ax.set_title('Learning rate\\ndecays from {:0.3g} to {:0.3g}'.format(y[0], y[-2]))\n","  ax.minorticks_on()\n","  ax.grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n","  ax.grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n","  ax.step(xx,y, linewidth=3, where='post')\n","  display.display(fig)\n","\n","class LogThirdLayerOutput(Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","        layer_output = get_layer_output(self.decoder_targets_one_hot)[0]\n","        print(layer_output.shape)\n","\n","class PlotTraining(Callback):\n","  def __init__(self, sample_rate=1, zoom=0.5):\n","    self.sample_rate = sample_rate\n","    self.step = 0\n","    self.zoom = zoom\n","    self.steps_per_epoch = NUM_SAMPLES//BATCH_SIZE\n","\n","  def on_train_begin(self, logs={}):\n","    self.batch_history = {}\n","    self.batch_step = []\n","    self.epoch_history = {}\n","    self.epoch_step = []\n","    self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 7))\n","    plt.ioff()\n","\n","  def on_batch_end(self, batch, logs={}):\n","    if (batch % self.sample_rate) == 0:\n","      self.batch_step.append(self.step)\n","      for k,v in logs.items():\n","        # do not log \"batch\" and \"size\" metrics that do not change\n","        # do not log training accuracy \"acc\"\n","        if k=='batch' or k=='size':# or k=='acc':\n","          continue\n","        self.batch_history.setdefault(k, []).append(v)\n","    self.step += 1\n","\n","  def on_batch_end(self, batch, logs={}):\n","    if (batch % self.sample_rate) == 0:\n","      self.batch_step.append(self.step)\n","      for k,v in logs.items():\n","        # do not log \"batch\" and \"size\" metrics that do not change\n","        # do not log training accuracy \"acc\"\n","        if k=='batch' or k=='size':# or k=='acc':\n","          continue\n","        self.batch_history.setdefault(k, []).append(v)\n","    self.step += 1\n","\n","  def on_epoch_end(self, epoch, logs={}):\n","    plt.close(self.fig)\n","    self.axes[0].cla()\n","    self.axes[1].cla()\n","      \n","    self.axes[0].set_ylim(0, 5/self.zoom)\n","    self.axes[1].set_ylim(0.3, 1+0.1/self.zoom/2)\n","    \n","    self.epoch_step.append(self.step)\n","    for k,v in logs.items():\n","      # only log validation metrics\n","      if not k.startswith('val_'):\n","        continue\n","      self.epoch_history.setdefault(k, []).append(v)\n","\n","    display.clear_output(wait=True)\n","    \n","    for k,v in self.batch_history.items():\n","      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.batch_step) / self.steps_per_epoch, v, label=k)\n","      \n","    for k,v in self.epoch_history.items():\n","      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.epoch_step) / self.steps_per_epoch, v, label=k, linewidth=3)\n","      \n","    self.axes[0].legend()\n","    self.axes[1].legend()\n","    self.axes[0].set_xlabel('epochs')\n","    self.axes[1].set_xlabel('epochs')\n","    self.axes[0].minorticks_on()\n","    self.axes[0].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n","    self.axes[0].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n","    self.axes[1].minorticks_on()\n","    self.axes[1].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n","    self.axes[1].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n","    display.display(self.fig)\n","    \n","plot_training = PlotTraining(sample_rate=10, zoom=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UVRSi0lTsp9","colab_type":"code","outputId":"1570bde3-f98f-4025-bda4-908bed2c9832","executionInfo":{"status":"ok","timestamp":1576628985568,"user_tz":300,"elapsed":174895,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["\n","link = selected_dic_model.get(\"LINK\")\n","fluff, model_id = link.split('=')\n","print (model_id) # Verify that you have everything after '='\n","downloaded = drive.CreateFile({'id':model_id}) \n","downloaded.GetContentFile('model.h5')\n","# load model\n","model = load_model('model.h5')\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["13kyuAcoULzFx2Hy3KOmlSDthGHXgdVMp\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jZCcskt0UTl0","colab_type":"code","outputId":"a294553c-f07d-46ca-ff92-147358d62778","executionInfo":{"status":"ok","timestamp":1576628985571,"user_tz":300,"elapsed":174888,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 21)           0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 11)           0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 21, 100)      1977800     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 11, 256)      6260992     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 256), (None, 365568      embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, 11, 256), (N 525312      embedding_2[0][0]                \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 11, 24457)    6285449     lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 15,415,121\n","Trainable params: 15,415,121\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VPgxQluZwoR3","colab_type":"code","outputId":"1073bfe3-9e37-4b00-ab0a-b3223dcb0d2b","executionInfo":{"status":"error","timestamp":1576628990748,"user_tz":300,"elapsed":180054,"user":{"displayName":"Estefan Apablaza","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZPpga1hpCDEI-dimGOTEmmE7zooqeHCJdi5K34g=s64","userId":"17250617738871791896"}},"colab":{"base_uri":"https://localhost:8080/","height":367}},"source":["from keras import optimizers\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","EPOCHS=100\n","dropout = 0.5\n","regul = 0.01\n","#optim = selected_dic_model.get(\"OPTIMIZER\")\n","#optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","#optim = optimizers.RMSprop(lr=0.001, rho=0.9)\n","optim = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n","if selected_dic_model.get(\"LINK\") is None: \n","  if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","    ##### build the model #####\n","    encoder_inputs_placeholder = Input(shape=(max_len_input,))\n","    x = embedding_layer(encoder_inputs_placeholder)\n","    encoder = LSTM(\n","      LATENT_DIM,\n","      return_state=True,\n","      dropout=dropout, # dropout not available on gpu,\n","      recurrent_regularizer=regularizers.l2(regul)\n","    )\n","    encoder_outputs, h, c = encoder(x)\n","    # encoder_outputs, h = encoder(x) #gru\n","\n","    # keep only the states to pass into decoder\n","    encoder_states = [h, c]\n","\n","  elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","    encoder_inputs_placeholder = Input(shape=(max_len_input,))\n","    x = embedding_layer(encoder_inputs_placeholder)\n","    encoder = GRU(\n","      LATENT_DIM,\n","      return_state=True,\n","      recurrent_regularizer=regularizers.l2(regul)\n","    )\n","    encoder_outputs, h = encoder(x) #gru\n","\n","    # keep only the states to pass into decoder\n","    encoder_states = [h]\n","\n","  decoder_inputs_placeholder = Input(shape=(max_len_target,))\n","\n","  # this word embedding will not use pre-trained vectors\n","  # although you could\n","  decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n","  decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n","  if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","    # since the decoder is a \"to-many\" model we want to have\n","    # return_sequences=True\n","    decoder_lstm = LSTM(\n","      LATENT_DIM,\n","      return_sequences=True,\n","      return_state=True,\n","      recurrent_regularizer=regularizers.l2(regul)\n","\n","    )\n","    decoder_outputs, _, _ = decoder_lstm(\n","      decoder_inputs_x,\n","      initial_state=encoder_states\n","    )\n","  elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","    # since the decoder is a \"to-many\" model we want to have\n","    # return_sequences=True\n","    decoder_gru = GRU(\n","      LATENT_DIM,\n","      return_sequences=True,\n","      return_state=True,\n","      dropout=dropout, # dropout not available on gpu\n","      recurrent_regularizer=regularizers.l2(regul)\n","\n","    )\n","    decoder_outputs, _ = decoder_gru(\n","      decoder_inputs_x,\n","      initial_state=encoder_states\n","    )\n","\n","  # final dense layer for predictions\n","  decoder_dense = Dense(num_words_output, activation='softmax')\n","  decoder_outputs = decoder_dense(decoder_outputs)\n","\n","  # Create the model object\n","  model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n","\n","  # Compile the model and train it\n","  model.compile(\n","    optimizer=optim,\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy'])\n","  early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","\n","  r = model.fit(\n","    [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n","    validation_split=0.2, \n","    callbacks=[plot_training,early_stopping])\n","\n","  # plot some data\n","  plt.plot(r.history['loss'], label='loss')\n","  plt.plot(r.history['val_loss'], label='val_loss')\n","  plt.legend()\n","  plt.show()\n","  # accuracies\n","  plt.plot(r.history['acc'], label='acc')\n","  plt.plot(r.history['val_acc'], label='val_acc')\n","  plt.legend()\n","  plt.show()\n","  import time\n","\n","  timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n","  filename = 'model_%s.h5' % timestr\n","  filename_r = 'model_%s.csv' % timestr\n","\n","  # Save model\n","  model.save(filename)\n","  model_file = drive.CreateFile({'title' : filename})\n","  model_file.SetContentFile(filename)  \n","  model_file.Upload()\n","  drive.CreateFile({'id': model_file.get('id')})\n","\n","  import pandas as pd\n","  # Save model\n","  hist_df = pd.DataFrame(r.history) \n","  hist_df.to_csv(filename_r)\n","  model_file_r = drive.CreateFile({'title' : filename_r})\n","  model_file_r.SetContentFile(filename_r)  \n","  model_file_r.Upload()\n","  drive.CreateFile({'id': model_file_r.get('id')})\n","else:\n","  link = selected_dic_model.get(\"LINK\")\n","  fluff, model_id = link.split('=')\n","  print (model_id) # Verify that you have everything after '='\n","  downloaded = drive.CreateFile({'id':model_id}) \n","  downloaded.GetContentFile('model.h5')\n","  # load model\n","  model = load_model('model.h5')\n","  encoder_inputs_placeholder = model.get_layer(name=\"input_9\").input\n","  decoder_inputs_placeholder = model.get_layer(name=\"input_10\").input\n","\n","  encoder_lstm = model.get_layer(name=\"lstm_3\")\n","  decoder_lstm = model.get_layer(name=\"lstm_4\")\n","\n","\n","  encoder_embedding = model.get_layer(\"embedding_1\")\n","  decoder_embedding = model.get_layer(\"embedding_3\")\n","\n","  decoder_dense = model.get_layer(\"dense_2\")\n","\n","  encoder_outputs, h, c = encoder_lstm(encoder_embedding.output)\n","  encoder_states = [h, c]\n","\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["13kyuAcoULzFx2Hy3KOmlSDthGHXgdVMp\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-299804f51f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0mencoder_inputs_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_9\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m   \u001b[0mdecoder_inputs_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No such layer: input_9"]}]},{"cell_type":"code","metadata":{"id":"lhnO7VRquJeF","colab_type":"code","colab":{}},"source":["  # Save model\n","  model.save(filename)\n","  model_file = drive.CreateFile({'title' : filename})\n","  model_file.SetContentFile(filename)  \n","  model_file.Upload()\n","  drive.CreateFile({'id': model_file.get('id')})\n","  \n","  import pandas as pd\n","  # Save model\n","  hist_df = pd.DataFrame(r.history) \n","  hist_df.to_csv(filename_r)\n","  model_file_r = drive.CreateFile({'title' : filename_r})\n","  model_file_r.SetContentFile(filename_r)  \n","  model_file_r.Upload()\n","  drive.CreateFile({'id': model_file_r.get('id')})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6DYvk9zsO-k","colab_type":"code","colab":{}},"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIhOF9NGfsXk","colab_type":"code","colab":{}},"source":["##### Make predictions #####\n","# As with the poetry example, we need to create another model\n","# that can take in the RNN state and previous word as input\n","# and accept a T=1 sequence.\n","\n","# The encoder will be stand-alone\n","# From this we will get our initial decoder hidden state\n","encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n","\n","\n","if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","  decoder_state_input_h = Input(shape=(LATENT_DIM,))\n","  decoder_state_input_c = Input(shape=(LATENT_DIM,))  \n","  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","  decoder_state_input_h = Input(shape=(LATENT_DIM,))\n","  decoder_states_inputs = [decoder_state_input_h] # gru\n","\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oXlH2GpVW_YY","colab":{}},"source":["\n","\n","# this time, we want to keep the states too, to be output\n","# by our sampling model\n","if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","  decoder_outputs, h, c = decoder_lstm(\n","    decoder_inputs_single_x,\n","    initial_state=decoder_states_inputs)\n","  decoder_states = [h, c]\n","\n","elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","  decoder_outputs, h = decoder_gru(\n","    decoder_inputs_single_x,\n","    initial_state=decoder_states_inputs )\n","  decoder_states = [h] # gru\n","\n","\n","decoder_outputs = decoder_dense(decoder_outputs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1IM789MeJCR","colab_type":"code","colab":{}},"source":["\n","# The sampling model\n","# inputs: y(t-1), h(t-1), c(t-1)\n","# outputs: y(t), h(t), c(t)\n","\n","decoder_model = Model([decoder_inputs_single] + decoder_states_inputs,[decoder_outputs]+ decoder_states)\n","\n","# map indexes back into real words\n","# so we can view the results\n","idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n","idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPJb6K-IWCsh","colab_type":"code","colab":{}},"source":["\n","def decode_sequence(input_seq):\n","  # Encode the input as state vectors.\n","  states_value = encoder_model.predict(input_seq)\n","  # Generate empty target sequence of length 1.\n","  target_seq = np.zeros((1, 1))\n","\n","  # Populate the first character of target sequence with the start character.\n","  # NOTE: tokenizer lower-cases all words\n","  target_seq[0, 0] = word2idx_outputs['<sos>']\n","\n","\n","  # if we get this we break\n","  eos = word2idx_outputs['<eos>']\n","\n","  # Create the translation\n","  output_sentence = []\n","  for _ in range(max_len_target):\n","    if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","      output_tokens, h, c = decoder_model.predict(\n","        [target_seq] + states_value\n","      )\n","    elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","      output_tokens, h = decoder_model.predict(\n","      [target_seq] + states_value ) # gru\n","\n","    # Get next word\n","    idx = np.argmax(output_tokens[0, 0, :])\n","\n","    # End sentence of EOS\n","    if eos == idx:\n","      break\n","\n","    word = ''\n","    if idx > 0:\n","      word = idx2word_trans[idx]\n","      output_sentence.append(word)\n","\n","    # Update the decoder input\n","    # which is just the word just generated\n","    target_seq[0, 0] = idx\n","\n","    # Update states\n","    if selected_dic_model.get(\"RNN\") == \"LSTM\":\n","      states_value = [h, c]\n","    elif selected_dic_model.get(\"RNN\") == \"GRU\":\n","      states_value = [h] # gru\n","\n","  return ' '.join(output_sentence)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlHE53z3WciA","colab_type":"code","colab":{}},"source":["sentences_5 = [\"He is the best\",\n","             \"Should have gotten a bike\",\n","             \"he doesnt really speak english\",\n","             \"Good luck penetrating German airspace\",\n","             \"YouTube is not being honest\",\n","             \"Preparing a taco in space\",\n","             \"Is that a legal solution\",\n","             \"School is really hard\",\n","             \"Today was a good day\",\n","             \"CNN and their debates\"]\n","for sentence in sentences_5 :           \n","  input_seq = tokenizer_inputs.texts_to_sequences(sentence.split())\n","  print(input_seq)\n","  input_seq_new = np.array(input_seq, ndmin=1).transpose() \n","  encoder_inputs_fake = pad_sequences(input_seq_new, maxlen=max_len_input)\n","  translation = decode_sequence(encoder_inputs_fake)\n","  states_value = encoder_model.predict(encoder_inputs_fake)\n","  print('Input: \"{}\":       Sarcasm: \"{}\"'.format(sentence,translation))\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cESmMZae2QRM","colab_type":"code","colab":{}},"source":["word = \" downvotes are because you\"\n","comment =data_clean[\"comment\"].str.contains(word) \n","parent_comment =data_clean[\"parent_comment\"].str.contains(word) \n","\n","print(data_clean[comment])\n","print(data_clean[parent_comment])"],"execution_count":0,"outputs":[]}]}
